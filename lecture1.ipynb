{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset: http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "# http://cs231n.stanford.edu/syllabus.html\n",
    "# decompose the dataset and \n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'r')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "# Extract training set and test set\n",
    "from numpy import *\n",
    "import glob\n",
    "\n",
    "dic_1 = unpickle(\"cifar10/data_batch_1\")\n",
    "training_set = dic_1[\"data\"]\n",
    "training_label = dic_1[\"labels\"]\n",
    "\n",
    "test_data = unpickle(\"cifar10/test_batch\")\n",
    "test_set = test_data[\"data\"]\n",
    "test_label = test_data[\"labels\"]\n",
    "\n",
    "for files in glob.glob(\"cifar10/data_batch*\"):\n",
    "    dic = unpickle(files)\n",
    "    if(files != \"cifar10/data_batch_1\"):\n",
    "        training_set = np.vstack((training_set, dic[\"data\"]))\n",
    "        training_label = np.hstack((training_label, dic[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Measurement\n",
    "\n",
    "A very important concept for nearest neighbor classfier is the distance measurement between two objects. There are two options: L1 and L2 distance.\n",
    "\n",
    "L1 distance, absolute value of elementwise subtraction and then all the differences are added up to a single var.\n",
    "\n",
    "\\begin{equation*}\n",
    "d_1(I_1, I_2)=\\sum_p |I_1^p - I_2^p|\n",
    "\\end{equation*}\n",
    "\n",
    "There are many other ways of computing distances between vectors. Another common choice could be to instead use the L2 distance, which has the geometric interpretation of computing the euclidean distance between two vectors. The distance takes the form:\n",
    "\n",
    "\\begin{equation*}\n",
    "d_1(I_1, I_2)=\\sqrt{ \\sum_p (I_1^p - I_2^p)^2 }\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NNClassifier(object):\n",
    "    # Do nothing in initialization state\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # train the classifier, uppercase parameter is a vector and lowercase parameter is a single element\n",
    "    def train(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "        \n",
    "    # test data set\n",
    "    def predict(self, X):\n",
    "        Ypred = np.zeros(X.shape[0], dtype = self.label.dtype)\n",
    "        # xrange only available in python 2.x\n",
    "        for i in xrange(X.shape[0]):\n",
    "            # L1 distance, axis = 0, summation of each column, 1 is row \n",
    "            dis = np.sum(np.abs(self.data - X[i,:]), axis = 1)\n",
    "            # L2 distance\n",
    "            # dis = np.sqrt(np.sum(np.squre(self.data - X[i, :])))\n",
    "            \n",
    "            # Returns the indices of the minimum values along an axis.\n",
    "            min_idx = np.argmin(dis)\n",
    "            Ypred[i] = self.label[min_idx]\n",
    "            \n",
    "        return Ypred\n",
    "    \n",
    "NNC = NNClassifier()\n",
    "NNC.train(training_set, training_label)\n",
    "pred_label = NNC.predict(test_set)\n",
    "print 'Accuracy: %f' % (np.mean(pred_label == test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of NNC\n",
    "\n",
    "If you ran this code, you would see that this classifier only achieves 24.9% on CIFAR-10. Thatâ€™s more impressive \n",
    "than guessing at random (which would give 10% accuracy since there are 10 classes), but nowhere near human \n",
    "performance or near state-of-the-art Convolutional Neural Networks that achieve about 95%.\n",
    "\n",
    "The advantage of NNC is that no need to train the system and easy implementation. Researchers can simply store and index the training data. While in Deep Neural Network, it takes a long  time to trian but it is very cheap to test different dataset. Nearest Neighbor Classifier is rarely appropriate for practical image classification. One problem is that images are high-dimensional objects and distances over high-dimensional spaces can be very counter-intuitive\n",
    "\n",
    "### Flow of Lecture\n",
    "Basically, lecture introduced the following topic:\n",
    "Image classsification --> NNC --> KNN --> Validation Set --> Cross Validation --> Evaluation --> NNC disadvantages\n",
    "\n",
    "Since I have learnt Machine Learning before and I want to spend more time on DNN and related tools. I don't show more details about the other topic. If you want to learn more about it, you can download related materials from the website provide at the beginning of this note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
